# Lambda Architecture using AWS Services 
The project integrates Batch processing and Stream processing for a Extract-Transform-Load Pipeline using AWS Services called from Command Line. 

# Table of Contents
1. [Overview of the System Architecture](#overview)
2. [Installation and Running](#installation-and-running)
3. [Test Scripts](#test-scripts) 

# Overview
## Introduction to Lambda Architecture
Lambda Architecture was an ETL Pipeline developed to unify batch processing and stream processing to improve the availability of data. Developed as a paradigm with no fixed software, Lambda Architecture gives Data Engineers the freedom to set up the pipeline using services and a cloud provider that best meets their requirements. Although Lambda Architecture has been around for over a decade, it serves as a good starting point for anyone venturing into data engineering. A follow-up to lambda architecture is Kappa Architecture and Delta Architecture. 

## Current Architecture 
Lambda Architecture is developed in three main layers: Batch, Streaming and Serving. The Batch and Streaming layers work independently and can be scaled depending on the incoming workload. The serving layer is a common dashboard to run queries and generate views on data collated from both sources as well as a visualisation dashboard. The below figure details the current workflow of the system. 

![lambdaArch](https://github.com/aanchal-n/Lambda-Architecture-AWS/blob/main/Assets/aws-lambda-architecture.png)

### Batch Layer 
The Batch Layer of the system is built to process large datasets of MBs or GBs. Batch Processing begins with the File Watcher script which scans a directory for incoming data files and on isolating them triggers the upload to the Raw S3 Bucket. From the raw S3 bucket, a lambda function invocation verifies the duplicity of files before invoking a Glue Crawler and a Glue Job. 

A Glue Crawler is used to generate tables that are added to a database called the "Glue Catalog". The Crawler runs on a specified source (here S3) and updates the table to account for any schema changes due to fresh incoming data. Tables generated by Glue Crawlers can be used to run Spark Jobs using a Glue Job or can be used to store data in a warehousing solution such as redshift. These are some of the key benefits of glue harnessed in this proof of concept. 

A Glue Job uses pyspark functions to transform the data before forwarding it down the pipeline. In the case of Raw data, the Glue Jobs function is to ensure unnecessary columns are dropped and necessary ones are created using existing data. In the case of Trusted data, Glue Jobs preprocesses the incoming data to refine them. This preprocessing could be in form of null value handling, duplicity check and so on. 

Once the data reaches the refined bucket, it is shifted to a data warehousing solution such as RedShift. The Refined bucket is connected to Athena to run smaller, ad-hoc queries. A connection to QuickShift allows the data to be visualised for Business Analysts to gain better insights. 

### Streaming Layer 
The Streaming Layer used the Kinesis Data Stream SDK to generate streams and push them through the pipeline. The Stream Generator script pushes the records into the Kinesis Data Stream which is connected to two different Kinesis Services. 

The Kinesis data stream pushes the records into Kinesis Firehose where the streams are transformed and stored as files in batches in a Streamed data S3 bucket. Once a defined number of records have been collected, a batch file is generated which is then pushed into the batch pipeline. The Streamed Data from the bucket is further visualised using a QuickSight dashboard for Business Analytics.

The Kinesis data stream is also connected to Kinesis Analytics. Kinesis Analytics uses Zeppelin notebooks to run Flink SQL queries to analyse the incoming stream data.

# Installation and Running 
To begin with, you can either clone this repository on your local system or download the zip file. Followed by which you run the below setup procedures. 
## Setup 
To access the AWS services, you require the boto3 package. Run the below command to automate this installation. 

``` pip3 install -r requirements.txt```

Followed by this, we need to setup our AWS services such as the IAM Role, S3 Buckets, Glue Crawlers and Jobs, and Kinesis Data Streams and Firehose. To automate this deployment, run the below command. 

```python3 setup.py```

The current version of this project relies on the user adding in their lambda functions and triggers to the AWS pipeline. This feature will be added soon. 

## Running the Pipeline
The pipeline can be run in two ways:
1. Batch ETL Pipeline
2. Lambda ETL Pipeline 

### Batch ETL Pipeline 
To run the pipeline to transform only batch data, we only require the file ingestor script to run as a daemon. To achieve this, you can execute the following command. 

```python3 File_Ingester.py```

### Lambda ETL Pipeline
To run the pipeline to account for both Streaming and Batch data, we use the stream ingestor which generates batch files once a certain record number is hit. To achieve this you can execute the below command. 

```python3 Kinesis-to-S3-Streaming.py```

# Test Scripts
Currently, the repository has Unit Tests for the Bucket Creation and Crawler Execution. In terms of handling pipeline breakdown, by default, most of the services are built to retry on failure. But in the case of record upload, if a certain records upload fails, it is documented and retried using logging. 

